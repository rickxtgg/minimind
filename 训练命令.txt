#单卡预训练命令

python train_pretrain.py --epoch 6 --batch_size 64
#多卡预训练命令

torchrun --nproc_per_node 2 train_pretrain.py --epoch 6 --batch_size 64

#单卡继续训练（根据实际情况选择数据集和继续训练的模型：dataset/distill_r1_110k_sft_converted_alt.jsonl，--resume --checkpoint_path out/pretrain_512.pth）
python train_pretrain.py --epoch 6 --batch_size 128 --data_path  dataset/distill_r1_110k_sft_converted_alt.jsonl --resume --checkpoint_path out/pretrain_512.pth

#多卡继续训练
torchrun --nproc_per_node 2 train_pretrain.py --epoch 10 --batch_size 128 --data_path  dataset/distill_r1_110k_sft_converted_alt.jsonl --resume --checkpoint_path out/pretrain_512.pth

#单卡监督训练命令
torchrun --nproc_per_node 2 train_full_sft.py --epoch 6 --batch_size 64

#多卡监督训练命令
torchrun --nproc_per_node 2 train_full_sft.py --epoch 6 --batch_size 32

#评估预训练模型"0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型"
python eval_model.py --model_mode 0

#评估sft监督训练模型
python eval_model.py --model_mode 1

#评估dpo微调训练模型
python eval_model.py --model_mode 2

#评估蒸馏微调训练模型
python eval_model.py --model_mode 3
